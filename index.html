<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ivona Najdenkoska</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë©üèº‚Äçüíª</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><b><span class="heading_color">Ivona</span></b> Najdenkoska</name>
              </p>
              <p>I'm a PhD student at the <a href="https://uva.nl">University of Amsterdam</a>, working with <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a> and <a href="https://yukimasano.github.io/">Yuki Asano</a>. I work on multimodal foundation models at the Informatics Institute, where I'm part of <a href="https://multix.io/">MultiX Amsterdam</a> and <a href="https://ivi.fnwi.uva.nl/aimlab/">AIMLab</a> groups.
              </p>
              <p>I am a member of the <a href="https://ellis.eu/">ELLIS Society</a> and have served as a reviewer for leading conferences, including CVPR, ICCV, NeurIPS, ICLR, and ICML.</p>
              <p style="text-align:center">
              <p>In 2023 I spent time as a Research Scientist Intern in <a href="https://ai.facebook.com/">Meta GenAI</a>, working on image generation and in-context learning.</p>
              <p>
                I obtained my master degree in Artificial Inteligence at the <a href="https://www.kuleuven.be/kuleuven">KU Leuven</a>. Before that, I spent some time as a Software Engineer in <a href="https://www.netcetera.com/home.html">Netcetera</a> and I was an undergraduate student in Computer Science and Engineering at the <a href="https://finki.ukim.mk/mk">FCSE</a> at University ‚ÄùSs. Cyril and Methodius‚Äù in Skopje.
              </p>
                <a href="mailto:i.najdenkoska@uva.nl">Email</a> &nbsp/&nbsp
                <a href="https://twitter.com/ivonajdenkoska">Twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2rFidrcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ivonajdenkoska/">Github</a>  &nbsp/&nbsp
                <a href="https://linkedin.com/in/ivona-najdenkoska/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ivona_photo_2.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ivona_photo_2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading><span class="heading_color">News</span></heading>
              <ul>
                <li>[Jul 2025] The preprint for LATTE‚òïÔ∏è is available on <a href="https://arxiv.org/pdf/2507.03054v1">arxiv</a>.</li>
                <li>[Apr 2025] I will be a TA for <a href="https://uvafomo.github.io/">Foundation Models (FoMo)</a> and Multimedia Analytics courses.</li>
                <li>[Feb 2025] TULIPüå∑ is accepted to ICLR 2025 üéâ</li>
                <li>[Oct 2024] Context Diffusion is accepted to ECCV 2024 üéâ</li>
                <li>[Apr 2024] I will be a TA for the <a href="https://uvafomo.github.io/">Foundation Models (FoMo)</a> course at UvA.</li>
                <li>[Dec 2023] The preprint of my internship work at Meta is available on <a href="https://arxiv.org/abs/2312.03584">arXiv</a>.</li>
                <li>[June 2023] I started a new position as a Research Scientist Intern at <a href="https://ai.facebook.com/">Meta AI</a> in Menlo Park, California üá∫üá∏</li>
                <li>[Mar 2023] I will be a TA for <a href="https://uvadl2c.github.io/">Deep Learning 2</a>,  Vison-Language learning module.</li>
                <li>[Jan 2023] Our paper on multimodal few-shot learning is accepted to ICLR 2023 üéâ</li>
                <li>[Nov 2022] I taught a guest lecture on Attention & Transformers, as part of the <a href="http://uvadlc.github.io/">Deep Learning 1</a> course.</li>
                <li>[Sept 2022] Our paper is runner-up for the <a href="https://www.sciencedirect.com/journal/medical-image-analysis/about/news">MEDIA Best Paper Award </a> at MICCAI 2022</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading><span class="heading_color">Research</span></heading>
              <p>
                My research centers around multimodal foundation models - with focus on designing efficient approaches for multimodal understanding and generative tasks. 
                I'm interested in better understanding what large-scale models learn, and how to exploit that through in-context learning and prompting. 
                Some of my work also includes automated linguistic interpretation of images, as well as its applications in the medical domain.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- Paper start -->
          <tr>
            <td style="padding-top:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/latte.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection</papertitle>
              <br>
              Ana Vasilcou*,
              <strong>Ivona Najdenkoska*</strong>,
              <a href="https://www.uva.nl/en/profile/g/e/z.j.m.h.geradts/z.j.m.h.geradts.html">Zeno Geradts</a>,
              <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a>
              <br>
              <em>Preprint (arxiv</em> 2025) &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2507.03054v1">paper</a> |
              <a href="https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector">code</a> 
        
              <p></p>
              <p>We present LATTE - Latent Trajectory Embedding - a novel approach for AI-generated image detection, which models the evolution of latent embeddings across several denoising timesteps.</p>
            </td>
          </tr> 
          <!-- Paper end -->

          <!-- Paper start -->
          <tr>
            <td style="padding-top:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/tulip_framework.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>TULIP: Token-length Upgraded CLIP</papertitle>
              <br>
              <strong>Ivona Najdenkoska</strong>*,
              <a href="https://mmderakhshani.github.io/">Mohammad M. Derakshani</a>*,
              <a href="https://yukimasano.github.io/">Yuki M. Asano</a>,
              <a href="https://nanne.github.io/">Nanne van Noord</a>,
              <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a>,
              <a href="https://www.ceessnoek.info/">Cees Snoek</a>
              <br>
              <em>In ICLR</em> 2025 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2410.10034">paper</a> | 
              <a href="https://github.com/ivonajdenkoska/tulip">code</a> |
              <a href="https://huggingface.co/datasets/mderakhshani/Long-DCI">Long-DCI benchmark</a>
              <p></p>
              <p>We propose TULIP, a generalizable method able to upgrade the token length to any length for CLIP-like models.</p>
            </td>
          </tr> 
          <!-- Paper end -->

          <!-- Paper start -->
          <tr>
            <td style="padding-top:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/context_diff_model.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Context Diffusion: In-Context Aware Image Generation</papertitle>
              <br>
              <strong>Ivona Najdenkoska</strong>,
              <a href="http://animesh-sinha.com/">Animesh Sinha</a>,
              <a href="https://scholar.google.com/citations?user=KJNUEgkAAAAJ">Abhimanyu Dubey</a>,
              <a href="https://scholar.google.com/citations?user=Gd9HQn2UsNoC&hl=en">Dhruv Mahajan</a>,
              <a href="https://scholar.google.com/citations?user=Zdl00bEAAAAJ&hl=en">Vignesh Ramanathan</a>,
              <a href="https://filipradenovic.github.io/">Filip Radenovic</a>
              <br>
              <em>In ECCV</em> 2024 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2312.03584.pdf">paper</a> | 
              <a href="https://ivonajdenkoska.github.io/contextdiffusion/main.html">project page</a>
        
              <p></p>
              <p>We present Context Diffusion, an in-context-aware image generation framework capable of learning from a variable number of visual context examples and prompts.</p>
            </td>
          </tr> 
          <!-- Paper end -->

          <!-- Paper start -->
          <tr>
            <td style="padding-top:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/secat.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Self-Supervised Open-Ended Classification with Small Visual Language Models</papertitle>
              <br>
              <a href="https://mmderakhshani.github.io/">Mohammad M. Derakshani</a>*,
              <strong>Ivona Najdenkoska*</strong>,
              <a href="https://www.ceessnoek.info/">Cees Snoek</a>,
              <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a>,
              <a href="https://yukimasano.github.io/">Yuki M. Asano</a>
              <br>
              <em>In ICLR ME-FoMo</em> 2024 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2310.00500.pdf">paper</a>         
              <p>We present Self-Context Adaptation (SeCAt), a selfsupervised approach that unlocks few-shot open-ended classification with small visual language models.</p>
            </td>
          </tr> 
          <!-- Paper end -->

          <!-- Paper start -->
          <tr>
            <td style="padding-top:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/metavl_model.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Meta Learning To Bridge Vision and Language Models for Multimodal Few-Shot Learning</papertitle>
              <br>
              <strong>Ivona Najdenkoska</strong>,
              <a href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Xiantong Zhen</a>,
              <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a>
              <br>
              <em>In ICLR</em> 2023 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2302.14794.pdf">paper</a> | 
              <a href="https://github.com/ivonajdenkoska/multimodal-meta-learn">code</a>
        
              <p></p>
              <p>We propose a method for bridging large-scale vision and language models to perform multimodal few-shot learning. The model meta-learns visual prefixes from frozen visual backbone, which are used as prompts to a large langauge model.</p>
            </td>
          </tr> 
          <!-- Paper end -->

          <tr>
            <td style="padding-top:20px;width:0%;vertical-align:middle">
              <div class="one">
                <img src='images/med_vqa_model.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=ovqQIMUAAAAJ&hl=en">Tom van Sonsbeek</a>*, <a href="https://mmderakhshani.github.io/">Mohammad M. Derakshani</a>*, <strong>Ivona Najdenkoska</strong>*, <a href="https://www.ceessnoek.info/">Cees Snoek</a>,
              <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a>
              <br>
              <em>In MICCAI</em> 2023, <span style="color: red">Oral</span>
              <br>
              <a href="https://arxiv.org/pdf/2303.05977.pdf">paper</a> | 
              <a href="https://github.com/tjvsonsbeek/open-ended-medical-vqa">code</a>
        
              <p></p>
              <p>We introduce a novel method for open-ended VQA suited for small, domain-specific, medical datasets. We employ parameter-efficient strategies for efficient tuning of the LMs. </p>
            </td>
          </tr> 

          <tr>
            <td style="padding-top:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/meta_setting.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Meta-Learning Makes a Better Multimodal Few-Shot Learner</papertitle>
              <br>
              <strong>Ivona Najdenkoska</strong>,
              <a href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Xiantong Zhen</a>,
              <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a>
              <br>
              <em>In 6th Workshop on Meta-Learning at NeurIPS</em> 2022 &nbsp 
              <br>
              <a href="https://openreview.net/pdf?id=7HPmTa_FdY">paper</a>
        
              <p></p>
              <p>We define a meta-learning approach for multimodal few-shot learning, to leverage its strong ability of accruing knowledge across tasks (predecessor of the ICLR 2023 work).</p>
            </td>
          </tr> 

          <tr>
            <td style="padding-left:20px;padding-bottom:30px;width:25%;vertical-align:middle>
              <div class="one">
                <img src='images/media_model.jpg' width="125", height="170">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Uncertainty-aware Report Generation for Chest X-rays by Variational Topic Inference</papertitle>
              <br>
              <strong>Ivona Najdenkoska</strong>,
              <a href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Xiantong Zhen</a>,
              <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a>, <a href="https://ling-shao.github.io/">Ling Shao</a>
              <br>
              <em>In Medical Image Analysis</em> 2022, <a href="https://www.sciencedirect.com/journal/medical-image-analysis/about/news" style="color: red">Best Paper Honorable Mention</a>
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S1361841522002341">paper</a> | 
              <a href="https://github.com/ivonajdenkoska/variational-xray-report-gen">code</a> 
              <p></p>
              <p>We present a probabilistic latent variable model for chest X-Ray report generation. We extend the VTI model by providing a fully Transformer-based definition and explore the trade-off between an LSTM- or Transformer-based decoder for generation of medical text. </p> </br></br>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/lifelonger_model.png' width="180">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>LifeLonger: A Benchmark for Continual Disease Classification</papertitle>
              <br>
              <a href="https://mmderakhshani.github.io/">Mohammad M. Derakshani</a>*, <strong>Ivona Najdenkoska</strong>*, 
              <a href="https://scholar.google.com/citations?user=ovqQIMUAAAAJ&hl=en">Tom van Sonsbeek</a>*,
              <a href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Xiantong Zhen</a>, 
              <a href="https://sites.google.com/site/dwarikanathmahapatra/">Dwarikanath Mahapatra</a>,
              <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a>, <a href="https://www.ceessnoek.info/">Cees Snoek</a> 
              <br>
              <em>In MICCAI</em> 2022
              <br>
              <a href="https://arxiv.org/pdf/2204.05737.pdf">paper</a> |
              <a href="https://github.com/mmderakhshani/LifeLonger">code</a> |
              <a href="https://lifelonger.github.io/">project page</a>
              <p></p>
              <p>We introduce LifeLonger, a benchmark for continual disease classification on the MedMNIST collection, by applying existing state-of-the-art continual learning methods.</p> </br></br>
            </td>
          </tr> 

          <tr>
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/midl_model.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Learning to Automatically Generate Accurate ECG Captions</papertitle>
              <br>
              Mathieu G. G. Bartels, <strong>Ivona Najdenkoska</strong>, Rutger van de Leur, Arjan Sammani, Karim Taha, David M. Knigge, Pieter Doevendans, <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a>, Rene van Es
              <br>
            <em>In MIDL</em> 2022
              <br>
              <a href="https://openreview.net/pdf?id=Y-kXbPYtzsg">paper</a> 
              <p></p>
              <p>We introduce a label-guided Transformer model, and show that it is possible to automatically generate relevant and readable ECG descriptions with a data-driven captioning model. </p> </br></br>
            </td>
          </tr> 

          <tr>
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/vti_model.png' width="200">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Variational Topic Inference for Chest X-Ray Report Generation</papertitle>
              <br>
              <strong>Ivona Najdenkoska</strong>, 
              <a href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Xiantong Zhen</a>,
              <a href="https://staff.fnwi.uva.nl/m.worring/">Marcel Worring</a>, <a href="https://ling-shao.github.io/">Ling Shao</a>
              <br>
            <em>In MICCAI</em> 2021, <span style="color: red">Oral + Travel Award</span>
              <br>
              <a href="https://arxiv.org/pdf/2107.07314.pdf">paper</a> |
              <a href="https://github.com/ivonajdenkoska/variational-xray-report-gen">code</a> 
              <p></p>
              <p>We propose Variational Topic Inference (VTI), a probabilistic latent variable model for automatic report generation. We introduce a set of topics as latent variables to guide sentence generation by aligning image and language modalities in the latent space. </p> </br></br>
            </td>
          </tr> 
          
        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading class="heading_color">Academic experience</heading>
            </td>
            <td width="95%" valign="middle">
              <dl>
                <dt><a href="https://uva.nl">University of Amsterdam</a> | Teaching Assistant in <a href="https://www.uva.nl/shared-content/programmas/en/masters/artificial-intelligence/artificial-intelligence.html">Master of AI</a> | Feb 2021 - present
                  <dl>
                     <dt>- <a href="https://uvafomo.github.io/">Foundation Models (2024, 2025)</a>, <a href="http://uvadlc.github.io/">Deep Learning 1 (2021, 2022)</a>, <a href="https://uvadl2c.github.io/">Deep Learning 2 (2023)</a>; Multimedia Analytics (2022, 2025); Information Visualization (2021).</dt>
                     <dt>- Guest lecturer for Deep Learning 1: Attention & Transformers.</dt>
                     <dt>- Supervision of Master in AI students during their thesis projects. 
                   </dt>
                   </dl>
                </dt>                
              </dl>
                              
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading class="heading_color">Work experience</heading>
            </td>
            <td width="95%" valign="middle">
              <dl>
                <dt> <a href="https://www.netcetera.com/home.html">Netcetera</a> | Software Engineer | Sept 2017 - Sept 2018
                </dt>
                <dt> <a href="https://www.netcetera.com/home.html">Netcetera</a> | Software Engineering Intern | Apr 2017 - Jul 2017
                </dt>
                <dt> <a href="https://www.haselt.com/">Haselt</a> | Software Engineering Intern | Jun 2016 - Sept 2016 
                </dt>
                            
              </dl>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading class="heading_color">Selected Talks</heading>
            </td>
            <td width="85%" valign="middle">
              <ul>  
                 <li>Invited talk @ DEX-XL on Gen AI in Deepfake Detection | March 2025, Noordwijkerhout</li>
                <li>Invited talk @ <a href="https://sites.google.com/view/nccv2024/home">NCCV 2024</a> about Context Diffusion paper | May 2024, Den Bosch.</li>
                <li>Invited talk @ <a href="https://www.core42.ai/">Core42</a> about Open-ended classification with small VLMs | Feb 2024, Abu Dhabi.</li>
                <li>Invited talk @ <a href="https://pbloem.github.io/fomo/">Foundation Model lecture series by UvA x VU</a> | June 2023, Amsterdam.</li>
                 <li>Invited talk @ <a href="https://amsterdammedicaldatascience.nl/">Amsterdam Medical Data Science (AMDS) meetup</a> | July 2022, Amsterdam.</li>   
                 <li>Participant @ DeepLearn Summer School, organized by IRDTA, | July 2022, Gran Canaria.</li>
                 <li>Participant @ Oxford Machine Learning Summer School, organized by AI for Global Goals | August 2021, Oxford - virtual.</li>
              </ul>
                              
            </td>
          </tr>
        </tbody></table>


      </td>
    </tr>
  </table>
  <div align="right"><small> Website template <a href="https://jonbarron.info/">here</a>. </small> </div>

</body>

</html>
